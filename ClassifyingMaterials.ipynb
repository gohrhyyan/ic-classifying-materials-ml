{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The 2025-2026 Computing Challenge: Machine Learning Classification for Materials Science\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This year the Computing Challenge revolves around writing code to build **machine learning classifiers** for materials science applications. Machine learning has become an essential tool in modern materials science, enabling researchers to predict material properties, discover new materials, and understand complex relationships between structure and function. Applications include **predicting material properties**, **accelerating materials discovery**, **optimizing synthesis conditions**, and **classifying materials based on their characteristics**.\n",
        "\n",
        "You are part of an engineering firm, Materials.AI.ML, whose main expertise is to analyse a client database to help them **reduce the cost of testing**. In this project, a client has 2 different requests, and associated dataset 1 and 2. Solving the client problem will require you to train, test and present the results of a dataset-specific algorithm.  \n",
        "\n",
        "1. **Dataset 1**: For each sample of a new 5-element alloy, you are given 11 features:\n",
        "\n",
        "1. Density (g/cm³)\n",
        "2. Vacancy content (fraction)\n",
        "3. Melting temperature (K)\n",
        "4. Heat conductivity (W/m·K)\n",
        "5. Band gap (eV)\n",
        "6. Crystallinity index\n",
        "7. Thermal expansion coefficient (1/K)\n",
        "8. Young's modulus (GPa)\n",
        "9. Hardness (GPa)\n",
        "10. Lattice parameter (Å)\n",
        "\n",
        "\n",
        "measured by the client. For each sample, the client also measured electrical conductivity, and then labelled the sample as conductive (1) or non-conductive. For unexplained reasons, measuring conductivity for these samples is extremely complicated and expensive, and they want an algorithm that is able to predict predict whether or not the sample will be conductive, or not. However, measuring the other features comes with a cost too, although it is much smaller (and the same for each feature). Based on your results, you have to **come up with a final recommendation** about which are the best features to measure to reduce costs yet retain the highest possible classification accuracy. \n",
        "\n",
        "2. **Dataset 2**: In a secret project, you are only given the values of 8 features measured for a new compound, and a label for classification. In this case, the client simply wants you to build an algorithm with the best possible classification accuracy. \n",
        "\n",
        "You will need to consider data analysis and preparation, then decide which classifier to train, evaluate their performance, and visualize the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Questions to Answer\n",
        "\n",
        "### For Dataset 1:\n",
        "\n",
        "1. **Can you build a Support Vector Machine (SVM) classifier that achieves perfect or near-perfect accuracy?** Since the data is linearly separable using the \"shiny\" feature, an SVM with a linear kernel should work well.\n",
        "\n",
        "2. **What is the importance of the \"shiny\" feature compared to other features?** You should investigate feature importance and verify that \"shiny\" is indeed the key separating feature.\n",
        "\n",
        "3. **How does the classifier performance change if you remove the \"shiny\" feature?** This will help you understand the role of this feature in classification.\n",
        "\n",
        "### For Dataset 2:\n",
        "\n",
        "1. **What classification algorithm works best for this multi-class spherical data problem?** You should try at least two different approaches (e.g., k-Nearest Neighbors, SVM with different kernels, or other classifiers).\n",
        "\n",
        "2. **What is the classification accuracy for each class?** You should report both overall accuracy and per-class accuracy.\n",
        "\n",
        "3. **How does the classifier performance change with the number of training samples?** Investigate the learning curve (accuracy vs. training set size).\n",
        "\n",
        "### General Questions:\n",
        "\n",
        "1. **How do you split your data into training and testing sets?** You should use appropriate train-test splitting (e.g., 80-20 or 70-30 split).\n",
        "\n",
        "2. **What metrics are appropriate for evaluating your classifiers?** Consider accuracy, precision, recall, F1-score, and confusion matrices.\n",
        "\n",
        "3. **How can you visualize high-dimensional data?** For Dataset 2 (8D), you'll need dimensionality reduction techniques like PCA (Principal Component Analysis) to visualize the data in 2D or 3D.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm 2: Data Preprocessing and Splitting\n",
        "\n",
        "Before training classifiers, you need to preprocess the data:\n",
        "\n",
        "> 1) **Normalize/Standardize features**: Different features may have very different scales (e.g., density in g/cm³ vs. melting temperature in K). Use `numpy` functions or implement standardization: $x_{\\\\rm std} = \\\\frac{x - \\\\mu}{\\\\sigma}$ where $\\\\mu$ is the mean and $\\\\sigma$ is the standard deviation.\n",
        "\n",
        "> 2) **Split into training and testing sets**: Use `numpy.random` to randomly split your data. A common split is 80% training and 20% testing. Make sure the split preserves class proportions (stratified splitting).\n",
        "\n",
        "> 3) **Handle the data**: Store training features, training labels, test features, and test labels separately.\n",
        "\n",
        "**Note**: You should use `numpy` arrays for all data storage and manipulation. Avoid using Python lists for large datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Suggestions and things to consider\n",
        "\n",
        "You need to implement classifiers for both datasets. While you may use machine learning libraries (like scikit-learn) for the actual classifier implementation, you should understand what they do and structure your code appropriately. Also, remember to use your brain and materials knowledge when considering what is the best course of action, or you risk wasting valuable resources. \n",
        "\n",
        "**In general, you should**:\n",
        "> 0) Make sense of the data and have a look at it before starting implementing an ML algorithm\n",
        "\n",
        "> 1) Clean the data if necessary. \n",
        "\n",
        "> 2) Avoid complexity if you can do something simple and still efficient. At equal efficiency, simpler == better. \n",
        "\n",
        "> 3) Always measure performance by using appropriate metrics (accuracy, precision, recall, F1-score).\n",
        "\n",
        "> 4) Investigate feature importance. You can do this by:\n",
        ">    - Training the classifier and examining the learned weights/coefficients\n",
        ">    - Training with and without certain features and comparing performance\n",
        "\n",
        "**Important**: You should structure your code using classes. For example, you might have:\n",
        "- A `Classifier` class (or separate classes for different classifier types)\n",
        "- An `Evaluator` class for computing metrics and visualizing results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation and Visualization\n",
        "\n",
        "You need to evaluate your classifiers and visualize the results:\n",
        "\n",
        "> 1) **Confusion matrices**: Create confusion matrices for both datasets to see which classes are confused with each other.\n",
        "\n",
        "> 2) **Learning curves**: For Dataset 2, plot accuracy as a function of training set size to understand how much data is needed.\n",
        "\n",
        "> 4) **Feature importance visualization**: For Dataset 1, create a bar plot showing the importance/weight of each feature in the classifier.\n",
        "\n",
        "**Note**: Use `matplotlib` for all visualizations. You may use libraries like scikit-learn for PCA, but you should understand what PCA does.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Results\n",
        "\n",
        "**Visualizing your data and results can be of great help!** Here are some example visualizations you might want to create:\n",
        "\n",
        "### Example: Visualizing Dataset 1 Features\n",
        "\n",
        "You can create scatter plots or correlation matrices to understand relationships between features:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Material to be Submitted\n",
        "\n",
        "> 1) **The code prepared to solve the challenge**, implementing classifiers, evaluation, and visualization\n",
        "\n",
        "> 2) **For Dataset 1**:\n",
        ">    - A written report providing your recommendation to the client regarding which measurements would reduce cost (see previous text), with an explanation for the rational for this recommendation. The latter might require using tables or plots, this is up to you.\n",
        "\n",
        "> 3) **For Dataset 2**:\n",
        ">    - Confusion matrices for each classifier you tested, with a note on any interesting observations or difficulties encountered, and why you think a given classifier worked best, if there is any.\n",
        ">    - A comparison plot showing accuracy of different classifiers tested\n",
        ">    - A learning curve plot (accuracy vs. training set size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission and Marking\n",
        "\n",
        "**Only a single person per group will have to submit the code and the plots on behalf of the whole group**. \n",
        "\n",
        "However: \n",
        "\n",
        "**each single person should separately submit a peer-review** of the contribution of different team members (more below). \n",
        "\n",
        "The submission must be done via Blackboard.\n",
        "\n",
        "The peer review evaluation, which can be done as a Word or text file, should be named \"peer-review evaluation\" and contain the names of your group components and, for each person, a mark of 0, 50 or 100 evaluating their contribution to the project (obviously, do not rate your own contribution...)\n",
        "\n",
        "The final total mark you will receive will be 70% of the group mark for the exercise + 30% coming from the peer evaluation. **If a person has more than a single peer evaluation of 0%, this person will have their total mark set to 0**, unless specific mitigation circumstances can be provided.\n",
        "\n",
        "### Marking Criteria\n",
        "\n",
        "> 1. Implementation of the overall Machine Learning pipeline and its different parts. The code should implement the various steps in a way that translates this problem into correct algorithms to solve it. This includes proper data analysis, train-test splitting, classifier training, and evaluation. **20 out of 100 Marks**\n",
        "\n",
        "> 2. **Appropriate structuring of the code**. The code must be organized into classes with clear responsibilities (e.g., DataGenerator, Classifier, Evaluator classes). **20 out of 100 Marks**\n",
        "\n",
        "> 3. **Use of the appropriate functionalities in scikit-learn**. **20 out of 100 Marks** \n",
        "\n",
        "> 4. Analysis of the results obtained, including the correct recommendation of the client. **40 out of 100 Marks** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some Remarks and How to Go About Solving This Challenge\n",
        "\n",
        "- There is **not** a single solution in terms of how to structure the code. However, a well-structured code will probably be divided into classes, with specific attributes. To build a step-by-step approach to the problem, you might have:\n",
        "  - A `Preprocessor` class that handles data cleaning / normalisation and train-test splitting\n",
        "  - A `BinaryClassifier` class (or a more general `Classifier` class)\n",
        "  - An `Evaluator` class that computes metrics and creates visualizations\n",
        "\n",
        "- In implementing the code, make use of libraries like scikit-learn to write compact code, you do not need to re-code things from scratch. \n",
        "\n",
        "- Before implementing any algorithm, take some time to reflect on which algorithm might be best, what assumptions different algorithms might make regarding the data, or even whether or not they are needed at all. The focus is understanding, not on code implementation (which should be almost trivial).\n",
        "\n",
        "- The general approach I would suggest is the following:\n",
        "\n",
        "1. First, **read the whole text of the problem once all together**. If you got to this point, you have basically done it already!\n",
        "2. Try and visualize the data provided to make sense of it. Consider how different features might be useful for making predictions, or not. \n",
        "3. Only after carefully considering the data, try implementing different algorithms. First, start with the simplest that make sense, given the data, do not go immediately to the most complex one! \n",
        "4. Once you trained your algorithms, always visualise the results. Visualisation helps understanding, and deciding the next step - or when to stop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
